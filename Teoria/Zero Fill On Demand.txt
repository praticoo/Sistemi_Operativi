PAGINE CONDIVISE
Condivisione di codice eseguibile: Diversi processi possono condividere la stessa area di memoria contenente il codice eseguibile di un programma. Questo è possibile perché il codice è di solito di sola lettura e non viene modificato durante l'esecuzione del programma. Ciò consente di risparmiare memoria poiché non è necessario duplicare il codice per ogni processo.
Memoria condivisa per IPC: I processi possono utilizzare la memoria condivisa per scambiarsi dati tra di loro. La memoria condivisa è una regione di memoria a cui più processi possono accedere per leggere e scrivere dati. Questa forma di condivisione di memoria è utile per la comunicazione e la sincronizzazione tra processi.
Implementazione tramite tabella delle pagine: L'implementazione delle pagine condivise può avvenire attraverso l'utilizzo di una tabella delle pagine ordinaria o multilivello. Questa tabella tiene traccia delle pagine condivise tra più processi e consente loro di accedere a queste pagine in modo efficiente. Questo approccio semplifica la gestione della memoria condivisa e permette di sfruttare al massimo la condivisione di risorse.

GESTIONE DELLA CACHE
La gestione della cache può presentare problemi di sincronizzazione quando si utilizzano cache basate su indirizzi virtuali, anche se vengono utilizzati gli ASID (Address Space Identifier) per distinguere tra diversi spazi di indirizzi. Questo perché le cache basate sugli indirizzi virtuali possono contenere duplicati di dati a causa della condivisione di pagine tra processi.
Per risolvere questi problemi, sono state proposte diverse soluzioni:
Disabilitare la cache sulle pagine condivise: Questa soluzione prevede la disabilitazione della cache per le pagine che vengono condivise tra i processi. In questo modo, si evitano i problemi di duplicazione dei dati nella cache. Tuttavia, ciò può comportare un aumento degli accessi alla memoria principale e una riduzione delle prestazioni complessive.
Utilizzare cache con ricerca basata su indirizzi virtuali e tag fisici (VIPT - Virtually Indexed Physically Tagged): In questo approccio, la cache effettua la ricerca in parallelo con la Translation Lookaside Buffer (TLB) utilizzando l'indirizzo virtuale. Una volta che l'indirizzo virtuale viene tradotto in un indirizzo fisico dalla TLB, il tag fisico viene utilizzato per verificare se i dati sono già presenti nella cache. Questo metodo riduce i problemi di duplicazione dei dati nella cache.
Tuttavia, in entrambi i casi, per determinare se si tratta di un duplicato, è necessario attendere che la TLB restituisca l'indirizzo fisico corrispondente all'indirizzo virtuale. Solo allora è possibile effettuare un confronto dei tag fisici nella cache. Questo può comportare un ritardo aggiuntivo nella gestione della cache.

TABELLA DELLE PAGINE INVERTITA
La tabella delle pagine invertita è un'alternativa alla tabella delle pagine tradizionale utilizzata nella gestione della memoria virtuale. Mentre nella tabella delle pagine ogni pagina virtuale ha un'entrata nella tabella, nella tabella delle pagine invertita ogni pagina fisica ha un'entrata nella tabella.
Nel contesto di un singolo core, l'alterazione della tabella delle pagine invertita può avvenire durante un context switch tra processi o in caso di page fault, quando è necessario caricare una pagina dalla memoria virtuale alla memoria fisica. Durante queste operazioni, le voci nella tabella delle pagine invertita vengono modificate per riflettere l'associazione corretta tra le pagine virtuali e le pagine fisiche.
Nel caso di sistemi multi-core, la gestione della tabella delle pagine invertita diventa più complessa. Poiché più core possono accedere contemporaneamente alla memoria, diventa difficile gestire le corrispondenze uno-a-uno tra pagine virtuali e pagine fisiche. In alcuni casi, possono essere necessarie tabelle delle pagine invertite teoriche con corrispondenze molti-a-uno per gestire efficacemente la condivisione delle pagine tra i diversi core. Questo può richiedere strategie avanzate per la sincronizzazione e la gestione coerente delle voci nella tabella delle pagine invertita tra i core.
In generale, la tabella delle pagine invertita può essere una soluzione efficace per gestire la memoria virtuale in sistemi con singolo core, ma può presentare sfide aggiuntive nella gestione della memoria in sistemi multi-core.

ZERO FILL ON DEMAND
Il meccanismo di "zero fill on demand" è un concetto legato alla gestione della memoria virtuale e alla gestione delle pagine condivise. Quando una pagina condivisa viene allocata in memoria virtuale (intendiamo la mappatura in memoria fisica), il suo contenuto iniziale può essere impostato a zero in modo efficiente, evitando la copia fisica dei dati dal disco o da altre pagine.
Questo meccanismo viene utilizzato per ridurre l'overhead di memoria e migliorare l'efficienza del sistema. Il meccanismo di "zero fill on demand" assegna inizialmente una pagina vuota (senza alcun dato) alla pagina condivisa. Solo quando un processo accede effettivamente alla pagina e scrive dei dati su di essa, viene generato un "fault di pagina" che attiva la copia fisica dei dati o la condivisione effettiva della pagina tra i processi.
In questo modo, si evita di dover allocare e inizializzare fisicamente tutte le pagine condivise in anticipo, ottimizzando l'utilizzo della memoria e riducendo il tempo di inizializzazione. Il meccanismo di "zero fill on demand" consente di allocare dinamicamente le pagine condivise solo quando sono effettivamente necessarie, riducendo così i requisiti di memoria fisica e migliorando le prestazioni complessive del sistema.

LIBRERIE 
Le librerie si riferiscono ai file di libreria dinamica o condivisa che contengono codice compilato e funzionalità che possono essere utilizzate da diversi programmi. Le librerie sono file binari che contengono implementazioni di funzioni, classi o altri componenti software che possono essere richiamati e utilizzati da uno o più programmi.
Quando un programma viene compilato, può fare riferimento a librerie esterne per accedere a funzionalità specifiche che non sono state implementate direttamente nel codice sorgente del programma. Queste librerie esterne vengono quindi collegate o caricate dinamicamente durante l'esecuzione del programma per fornire le funzionalità richieste.
Le librerie possono includere librerie di sistema, librerie di terze parti o librerie personalizzate sviluppate internamente. Esistono diverse forme di librerie, come le librerie statiche (.lib su Windows, .a su Unix) che vengono collegate direttamente al programma durante la compilazione, e le librerie dinamiche o condivise (.dll su Windows, .so su Unix) che vengono caricate dinamicamente durante l'esecuzione del programma.

LIBRERIE CONDIVISE 
Le grandi librerie condivise sono un'importante componente del software moderno. Consentono di condividere e riutilizzare il codice tra diversi programmi, offrendo vantaggi come il risparmio di spazio su disco e in RAM, lo sviluppo indipendente e la facilità di aggiornamento. Ci sono due approcci principali per l'utilizzo di librerie condivise: il linking statico e il linking dinamico.
Nel caso del linking statico, il codice della libreria viene incluso direttamente nel programma durante la fase di linking. Questo significa che tutto il codice della libreria viene incorporato nel file eseguibile del programma finale. Ciò garantisce che tutte le dipendenze siano soddisfatte e che il programma possa essere eseguito autonomamente senza dipendere da librerie esterne. Tuttavia, questo approccio può portare a un aumento della dimensione del file eseguibile e alla duplicazione del codice se più programmi utilizzano la stessa libreria.
Nel caso del linking dinamico, le librerie condivise vengono collegate e caricate a runtime durante l'esecuzione del programma. Il programma fa riferimento alle librerie condivise esterne e il collegamento avviene quando il programma viene avviato. Questo consente di ridurre lo spazio occupato sul disco e in RAM, in quanto le librerie condivise possono essere utilizzate da più programmi senza dover duplicare il codice. Inoltre, il linking dinamico offre la possibilità di aggiornare le librerie condivise in modo indipendente dai programmi che le utilizzano, semplificando l'aggiornamento del software.

FILE MAPPATI
I file mappati in memoria sono un modello alternativo di I/O su file che offre diverse vantaggi. Invece di utilizzare le tradizionali operazioni di lettura e scrittura per accedere ai dati di un file, i file mappati in memoria consentono di mappare il contenuto di un file direttamente nella memoria del processo.
Ciò significa che il file viene trattato come se fosse una porzione di memoria del processo. Questo permette un accesso molto più veloce ai dati del file, in quanto non è necessario effettuare chiamate di sistema per ogni operazione di I/O. Invece, il processo può accedere direttamente alla memoria mappata e i dati sono trasferiti tra la memoria e il file in modo trasparente.
Una delle principali caratteristiche dei file mappati in memoria è la possibilità di condivisione. Più processi possono mappare lo stesso file contemporaneamente e condividere i dati in esso contenuti. Questo è particolarmente utile quando più processi devono accedere agli stessi dati, ad esempio per la condivisione di dati tra processi o per l'accesso concorrente a un database.
Inoltre, i file mappati in memoria gestiscono automaticamente il caricamento di librerie condivise e del codice eseguibile. Ad esempio, se un file mappato contiene una libreria condivisa necessaria per l'esecuzione di un programma, il sistema operativo si occupa automaticamente di caricare la libreria in memoria quando il file viene mappato.
Infine, i file mappati in memoria consentono anche il caricamento dei dati statici. Ad esempio, se un file contiene dati statici che devono essere accessibili da più processi, è possibile mappare il file in memoria e i dati saranno disponibili a tutti i processi che mappano il file.
In conclusione, i file mappati in memoria offrono un modo efficiente e veloce per accedere ai dati di un file, consentono la condivisione dei dati tra processi e semplificano il 
caricamento di librerie condivise e dati statici.

ALLOCAZIONE DI MEMORIA DEL KERNEL
Quando un processo eseguito in modalità utente necessita di memoria aggiuntiva, le pagine sono allocate dalla lista dei frame disponibili mantenuta dal kernel. Per formare questa lista, si applica in genere uno degli algoritmi di sostituzione delle pagine; molto verosimilmente, la lista conterrà pagine non utilizzate sparse per tutta la memoria.
Va inoltre ricordato che, se un processo utente richiede un solo byte di memoria, si ottiene frammentazione interna, poiché al processo viene garantito un intero frame.
Il kernel, per allocare la propria memoria, attinge spesso a una riserva di memoria libera differente dalla lista usata per soddisfare gli ordinari processi in modalità utente. 
Questo avviene principalmente per due motivi.
1. Il kernel richiede memoria per strutture dati dalle dimensioni variabili; alcune di loro corrispondono a meno di una pagina. Deve quindi fare un uso oculato della
memoria, tentando di contenere al minimo gli sprechi dovuti alla frammentazione.
Questo fattore è di particolare rilevanza, se si considera che, in molti sistemi operativi, il codice e i dati del kernel non sono paginabili.
Questo significa che queste porzioni di memoria devono essere mantenute residenti nella memoria principale, senza essere spostate nella memoria di swap o nella memoria virtuale paginata.
Situazioni critiche 
Gestione delle interruzioni: Durante la gestione di un'interruzione, è fondamentale che il kernel abbia accesso immediato alle informazioni necessarie per rispondere all'evento di interruzione. Se queste informazioni fossero paginate, potrebbe verificarsi un ritardo nell'accesso ai dati critici, influenzando negativamente la risposta dell'interruzione. Pertanto, alcune porzioni di memoria interne al kernel potrebbero essere mantenute residenti nella memoria principale per garantire tempi di risposta rapidi.
Gestione delle eccezioni: Situazioni eccezionali come errori di pagina, violazioni di accesso o errori di divisione richiedono una gestione immediata e accurata da parte del kernel. Per facilitare una risposta rapida e precisa a queste eccezioni, alcune porzioni di memoria interna al kernel potrebbero essere mantenute residenti nella memoria principale.
Sicurezza: In alcune situazioni in cui è necessario proteggere le informazioni sensibili o riservate all'interno del kernel, potrebbe essere preferibile mantenere tali dati nella memoria principale per evitare il rischio di esposizione tramite la memoria di swap o la paginazione.
2. Le pagine allocate ai processi in modalità utente non devono necessariamente essere contigue nella memoria fisica. Alcuni dispositivi, però, interagiscono direttamente con la memoria fisica, senza il vantaggio dell’interfaccia della memoria virtuale; di conseguenza, possono richiedere memoria che risieda in pagine fisicamente contigue.

SLAB ALLOCATION - ALLOCAZIONE A LASTRE 
Una lastra è composta da una o più pagine fisicamente contigue.
Una cache consiste di una o più lastre.
Vi è una sola cache per ciascuna categoria di struttura dati del kernel: una cache dedicata alla struttura dati che rappresenta i descrittori dei processi (strutture dati che contengono informazioni sullo stato e sulle risorse assegnate a un processo in esecuzione. Queste informazioni includono il PID (Process IDentifier), che identifica univocamente il processo all'interno del sistema operativo, il PPID (Parent Process IDentifier), che identifica il processo padre del processo corrente, lo stato del processo (in esecuzione, in attesa, terminato, etc.), le risorse assegnate al processo (come la memoria, i file aperti...), e altre informazioni utili per la gestione del processo da parte del sistema operativo), una dedicata agli oggetti che rappresentano i file, un’altra per i semafori, e così via.
Ogni cache è popolata da oggetti, istanze della struttura dati del kernel rappresentata dalla cache. La cache che rappresenta i semafori, per esempio,
memorizza istanze di oggetti semaforo; quella che rappresenta i descrittori dei processi memorizza istanze di descrittori dei processi, e così via. 
La relazione fra lastre, cache e oggetti è illustrata; essa mostra due oggetti del kernel che misurano 3 kB e tre oggetti che misurano 7 kB, memorizzati nelle rispettive cache.
L’algoritmo di allocazione a lastre utilizza le cache per memorizzare oggetti del kernel. Quando si crea una cache, un certo numero di oggetti, inizialmente dichiarati
liberi, viene assegnato alla cache. Questo numero dipende dalla grandezza della lastra associata alla cache. Per esempio, una lastra di 12 kB (formata da tre pagine contigue
di 4 kB) potrebbe contenere sei oggetti di 2 kB ciascuno. Al principio, tutti gli oggetti nella cache sono contrassegnati come liberi. Quando una struttura dati del kernel ha
bisogno di un oggetto, per soddisfare la richiesta l’allocatore può selezionare dalla cache qualunque oggetto libero; l’oggetto tratto dalla cache è quindi contrassegnato
come usato.
In Linux una lastra può essere in uno dei seguenti stati.
1. Piena. Tutti gli oggetti della lastra sono contrassegnati come usati.
2. Vuota. Tutti gli oggetti della lastra sono contrassegnati come liberi.
3. Parzialmente occupata. La lastra contiene oggetti sia usati sia liberi.
L’allocatore a lastre, per soddisfare una richiesta, tenta in primo luogo di estrarre un oggetto libero da una lastra parzialmente occupata; se non ne esistono, assegna un 
oggetto libero da una lastra vuota; in mancanza di lastre vuote disponibili, crea una nuova lastra da pagine fisiche contigue e la alloca a una cache; da tale lastra si attinge
la memoria da allocare all’oggetto.
L’allocatore a lastre offre due vantaggi principali.
1. Annulla lo spreco di memoria derivante da frammentazione. La frammentazione non è un problema, poiché ogni struttura dati del kernel ha una cache associata;
ciascuna delle cache è composta da un numero variabile di lastre, suddivise in spezzoni di grandezza pari a quella degli oggetti rappresentati. Pertanto, quando
il kernel richiede memoria per un oggetto, l’allocatore a lastre restituisce la quantità esatta di memoria necessaria per rappresentare l’oggetto.
2. Le richieste di memoria possono essere soddisfatte rapidamente. La tecnica di allocazione a lastre si rivela particolarmente efficace quando, nella gestione della
memoria, gli oggetti sono frequentemente allocati e deallocati, come spesso accade con le richieste del kernel. In termini di tempo, allocare e deallocare memoria
può essere un processo dispendioso. Tuttavia, gli oggetti sono creati in anticipo e possono dunque essere allocati rapidamente dalla cache. Inoltre, quando il kernel
rilascia un oggetto di cui non ha più bisogno, questo è dichiarato libero e restituito alla propria cache, rendendolo così immediatamente disponibile ad altre richieste
del kernel.
